{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader, Subset, Dataset\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet34, resnet18, resnet50, alexnet, googlenet\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time \n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "import warnings\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, ReduceLROnPlateau\n",
    "import random\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.utils as utils\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import wave\n",
    "import pylab\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from matplotlib.colors import Normalize\n",
    "import itertools\n",
    "from scipy.signal import spectrogram, find_peaks\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import os\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms.functional import invert\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CustomModel\"\n",
    "explained_variance = 0.8\n",
    "f_selected = 6500\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/\"\n",
    "minor_folder = os.path.join(data_dir, \"Minor\")\n",
    "major_folder = os.path.join(data_dir, \"Major\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"PCA_f{f_selected}_expVar{explained_variance}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dir = \"./data/audio-images/\"\n",
    "matching_directory = [d for d in os.listdir(folder_dir) if os.path.isdir(os.path.join(folder_dir, d)) and d.startswith(prefix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/audio-images/PCA_f6500_expVar0.8_ncompMinor36_ncompMajor36 already present.\n"
     ]
    }
   ],
   "source": [
    "images_dir = os.path.join(data_dir, 'audio-images', matching_directory[0])\n",
    "if not os.path.exists(images_dir):\n",
    "    print(f'{images_dir} is not present.')\n",
    "else:\n",
    "    print(f'{images_dir} already present.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'val', 'train']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Major', 'Minor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(images_dir, \"train\"); val_path = os.path.join(images_dir, \"val\"); test_path = os.path.join(images_dir, \"test\")\n",
    "train_dataset = datasets.ImageFolder(train_path)\n",
    "val_dataset = datasets.ImageFolder(val_path)\n",
    "test_dataset = datasets.ImageFolder(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transformations_train = [\n",
    "    transforms.Compose([v2.Resize((224, 224), antialias='True'), transforms.ToTensor()]),\n",
    "    transforms.Compose([v2.Resize((224, 224), antialias='True'), transforms.ToTensor(),  transforms.Grayscale(num_output_channels=3)]),\n",
    "    transforms.Compose([v2.Resize((224, 224), antialias='True'), invert, transforms.ToTensor()]),\n",
    "    transforms.Compose([ v2.Resize((224, 224), antialias='True'), v2.RandomErasing(p=1, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=255), transforms.ToTensor()]),\n",
    "]\n",
    "\n",
    "custom_transformations_val_test = [\n",
    "    transforms.Compose([v2.Resize((224, 224), antialias='True'), transforms.ToTensor()]),#, v2.Normalize(mean, stdev)]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, augmented_data, transform=None):\n",
    "        self.augmented_data = augmented_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.augmented_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.augmented_data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(dataset, custom_transformations):\n",
    "    augmented_data = []\n",
    "    for i in range(len(dataset)):\n",
    "        original_img, target = dataset[i] #image (not tensor)\n",
    "        for idx, augment_transform in enumerate(custom_transformations):\n",
    "            augmented_img = augment_transform(original_img) #output is a tensor\n",
    "            augmented_img = transforms.ToPILImage()(augmented_img)\n",
    "            augmented_data.append((augmented_img, target))\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_dataset = transform_dataset(train_dataset, custom_transformations_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_val_dataset = transform_dataset(val_dataset, custom_transformations_val_test)\n",
    "augmented_test_dataset = transform_dataset(test_dataset, custom_transformations_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_dataset = AugmentedDataset(augmented_train_dataset, transform= transforms.ToTensor())\n",
    "augmented_val_dataset = AugmentedDataset(augmented_val_dataset, transform= transforms.ToTensor())\n",
    "augmented_test_dataset = AugmentedDataset(augmented_test_dataset, transform= transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(augmented_train_dataset, batch_size=batch_size, num_workers=2, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(augmented_val_dataset, batch_size=batch_size, num_workers=2, pin_memory=True, shuffle=True)\n",
    "test_loader = DataLoader(augmented_test_dataset, batch_size=batch_size, num_workers=2, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_accuracy_list, train_loss_list, val_accuracy_list, val_loss_list, learning_rate_list, model_name):\n",
    "\n",
    "    results_dict = f'./results/{model_name}_batchsize{batch_size}_PCA_f{f_selected}_expVar{explained_variance}'\n",
    "    if not os.path.exists(results_dict):\n",
    "        os.makedirs(results_dict)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    axs[0].plot(train_loss_list, label='Training Loss')\n",
    "    axs[0].plot(val_loss_list, label='Validation Loss')\n",
    "    axs[0].set_title('Loss Curves')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "\n",
    "    axs_lr1 = axs[0].twinx()\n",
    "    axs_lr1.semilogy(np.arange(len(learning_rate_list)), learning_rate_list, 'r--', label='Learning Rate')\n",
    "    axs_lr1.set_ylabel('Learning Rate')\n",
    "    handles1, labels1 = axs[0].get_legend_handles_labels()\n",
    "    handles_lr1, labels_lr1 = axs_lr1.get_legend_handles_labels()\n",
    "    handles1.extend(handles_lr1)\n",
    "    labels1.extend(labels_lr1)\n",
    "    \n",
    "    axs[1].plot(train_accuracy_list, label='Training Accuracy')\n",
    "    axs[1].plot(val_accuracy_list, label='Validation Accuracy')\n",
    "    axs[1].set_title('Accuracy Curves')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].set_ylim([0, 110])\n",
    "\n",
    "    axs_lr2 = axs[1].twinx()\n",
    "    axs_lr2.semilogy(np.arange(len(learning_rate_list)), learning_rate_list, 'r--', label='Learning Rate')\n",
    "    axs_lr2.set_ylabel('Learning Rate')\n",
    "    handles2, labels2 = axs[1].get_legend_handles_labels()\n",
    "    handles_lr2, labels_lr2 = axs_lr2.get_legend_handles_labels()\n",
    "    handles2.extend(handles_lr2)\n",
    "    labels2.extend(labels_lr2)\n",
    "\n",
    "    axs[0].legend(handles1, labels1, loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=3)\n",
    "    axs[1].legend(handles2, labels2, loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=3)\n",
    "\n",
    "    fig.suptitle(fr'Learning Curves - {model_name}, $f_{{sel}}: {f_selected}$ Hz, Batch size: {batch_size}, Explained variance: {explained_variance*100}%', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_dict}/learningCurves_{model_name}_batchsize{batch_size}_PCA_f{f_selected}_expVar{explained_variance}.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, criterion, title): \n",
    "    results_dict = f'./results/{model_name}_batchsize{batch_size}_PCA_f{f_selected}_expVar{explained_variance}'\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5)) \n",
    "    all_preds = []; all_labels = []; all_probs = []\n",
    "    model.eval()\n",
    "    running_loss = 0.0; running_corrects = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.permute(0,1,3,2)\n",
    "        inputs = inputs.to(device); labels = labels.to(device)\n",
    "        loss, preds, outputs = get_loss_preds(model, criterion, inputs, labels) \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())  \n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        all_probs.append(probabilities.detach().cpu().numpy())\n",
    "    \n",
    "    loss = running_loss / len(loader.dataset)\n",
    "    acc = 100*running_corrects.double().item() /len(loader.dataset)\n",
    "    print(f'Loss: {loss}, Accuracy: {acc}')\n",
    "    \n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds, normalize='true')\n",
    "\n",
    "    sns.heatmap(conf_matrix,  cmap=\"YlGnBu\", annot=True, cbar=False,\n",
    "                xticklabels=class_names, yticklabels=class_names, ax = axs[0])\n",
    "    axs[0].set_xlabel('Predicted labels')\n",
    "    axs[0].set_ylabel('True labels')\n",
    "    axs[0].set_title(f'Confusion Matrix')\n",
    "    #plt.show()\n",
    "    \n",
    "    binarized_labels = all_labels\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(binarized_labels, all_probs[:, 1])\n",
    "    axs[1].plot(recall, precision, lw=2, label = 'Precision-Recall curve')\n",
    "\n",
    "    axs[1].set_xlabel('Recall')\n",
    "    axs[1].set_ylabel('Precision')\n",
    "    axs[1].set_title('Precision-Recall Curve')\n",
    "    axs[1].legend()\n",
    "    plt.grid(True)\n",
    "    plt.suptitle( title + fr' {model_name}, $f_{{sel}}: {f_selected}$ Hz, Batch size: {batch_size}, Explained variance: {explained_variance*100}%' \"\\n\" \n",
    "                    fr'Test Loss: {loss:.3f}, Test Accuracy: {acc:.3f}%', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_dict}/{title}_confusion_PRC_{model_name}_batchsize{batch_size}_PCA_f{f_selected}_expVar{explained_variance}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_preds(model, criterion, inputs, labels):\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    loss = criterion(outputs, labels)\n",
    "    return loss, preds, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, model_name = \"CustomModel\"):\n",
    "    since = time.time()\n",
    "\n",
    "    checkpoint_dir_prefix = f'./checkpoints/{model_name}_batchsize{batch_size}_PCA_f{f_selected}_expVar{explained_variance}'\n",
    "    checkpoint_dir = f'./checkpoints/{model_name}_batchsize{batch_size}_PCA_f{f_selected}_expVar{explained_variance}/checkpoint.pth'\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir_prefix):\n",
    "        os.makedirs(checkpoint_dir_prefix)\n",
    "    \n",
    "    files_to_delete = [f for f in os.listdir(checkpoint_dir_prefix) if os.path.isfile(os.path.join(checkpoint_dir_prefix, f))]\n",
    "\n",
    "    if os.path.exists(checkpoint_dir_prefix) and os.path.getsize(checkpoint_dir_prefix) > 0:\n",
    "        if (files_to_delete):\n",
    "            file_path = os.path.join(checkpoint_dir_prefix, 'checkpoint.pth')\n",
    "            os.remove(file_path)\n",
    "    \n",
    "    phases = [\"train\", \"val\"]; loaders = {\"train\":train_loader, \"val\":val_loader}\n",
    "    train_accuracy_list = []; val_accuracy_list = []; train_loss_list = []; val_loss_list = []; learning_rate_list = []\n",
    "    \n",
    "    layer_adjustment_epochs = []\n",
    "    \n",
    "    for phase in phases:\n",
    "        running_loss = 0.0; running_corrects = 0\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for inputs, labels in loaders[phase]:\n",
    "            inputs = inputs.permute(0,1,3,2)\n",
    "            inputs = inputs.to(device); labels = labels.to(device)\n",
    "            loss, preds, outputs = get_loss_preds(model, criterion, inputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss /len(loaders[phase].dataset)\n",
    "        epoch_acc = 100*running_corrects.double().item() /len(loaders[phase].dataset)\n",
    "\n",
    "        if (phase==\"train\"):\n",
    "            train_accuracy_list.append(epoch_acc)\n",
    "            train_loss_list.append(epoch_loss)\n",
    "        elif (phase==\"val\"):\n",
    "            val_accuracy_list.append(epoch_acc)\n",
    "            val_loss_list.append(epoch_loss)\n",
    "            best_acc = epoch_acc\n",
    "    learning_rate_list.append(optimizer_ft.param_groups[0][\"lr\"])\n",
    "    print(f'Epoch {0}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "    print(f'Training Loss: {train_loss_list[0]:.4f}, Validation Loss: {val_loss_list[0]:.4f}, Training Acc: {train_accuracy_list[0]:.4f}, Validation Acc: {val_accuracy_list[0]:.4f}, Learning Rate: {learning_rate_list[0]}')\n",
    "    \n",
    "    print(\"Model saved.\")\n",
    "    torch.save({\n",
    "    'epoch': 0,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in phases:\n",
    "            running_loss = 0.0; running_corrects = 0\n",
    "            if (phase==\"train\"):\n",
    "                model.train()\n",
    "            elif (phase==\"val\"):\n",
    "                model.eval()\n",
    "            for inputs, labels in loaders[phase]:\n",
    "                inputs = inputs.permute(0,1,3,2)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss, preds, outputs = get_loss_preds(model, criterion, inputs, labels)\n",
    "                if (phase==\"train\"):\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            epoch_loss = running_loss / len(loaders[phase].dataset)\n",
    "            epoch_acc = 100*running_corrects.double().item() /len(loaders[phase].dataset)\n",
    "            \n",
    "            if (phase==\"train\"):\n",
    "                train_accuracy_list.append(epoch_acc)\n",
    "                train_loss_list.append(epoch_loss)\n",
    "            elif (phase==\"val\"):\n",
    "                val_acc = epoch_acc\n",
    "                val_accuracy_list.append(epoch_acc)\n",
    "                val_loss_list.append(epoch_loss)   \n",
    "        scheduler.step()\n",
    "        \n",
    "        learning_rate_list.append(optimizer_ft.param_groups[0][\"lr\"])\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        print(f'Training Loss: {train_loss_list[epoch+1]:.4f}, Validation Loss: {val_loss_list[epoch+1]:.4f}, Training Acc: {train_accuracy_list[epoch+1]:.4f}, Validation Acc: {val_accuracy_list[epoch+1]:.4f}, Learning Rate: {learning_rate_list[epoch+1]}')\n",
    "        \n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Model saved.\")\n",
    "            torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_dir)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "    return model, train_accuracy_list, train_loss_list, val_accuracy_list, val_loss_list, learning_rate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_classes, IMAGE_HEIGHT, IMAGE_WIDTH):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * (IMAGE_HEIGHT // 16) * (IMAGE_WIDTH // 16), 256)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (model_name == \"ResNet18\"):\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "elif (model_name == \"ResNet34\"):\n",
    "    model = resnet34(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "elif (model_name == \"CustomModel\"):\n",
    "    model = CustomModel(num_classes=num_classes, IMAGE_HEIGHT =224, IMAGE_WIDTH = 224)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, weight_decay, epochs = 1e-4, 1e-3, 20\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer_ft, step_size=5, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_accuracy_list, train_loss_list, val_accuracy_list, val_loss_list, learning_rate_list = train_model(model, criterion, optimizer_ft, scheduler, num_epochs=epochs, model_name = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(train_accuracy_list, train_loss_list, val_accuracy_list, val_loss_list, learning_rate_list, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = f'./checkpoints/{model_name}_batchsize{batch_size}_PCA_f{f_selected}_expVar{explained_variance}/checkpoint.pth'\n",
    "\n",
    "if os.path.exists(checkpoint_dir) and os.path.getsize(checkpoint_dir) > 0:\n",
    "    if (model_name == \"ResNet18\"):\n",
    "        model = resnet18(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "    elif (model_name == \"ResNet34\"):\n",
    "        model = resnet34(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "    elif (model_name == \"CustomModel\"):\n",
    "        model = CustomModel(num_classes=num_classes, IMAGE_HEIGHT =224, IMAGE_WIDTH = 224)\n",
    "    model = model.to(device)\n",
    "\n",
    "    title = 'Raw'\n",
    "    evaluate_model(model, test_loader, criterion, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_dir)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "title = \"Fine-tuned\"\n",
    "evaluate_model(model, test_loader, criterion, title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE499",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
